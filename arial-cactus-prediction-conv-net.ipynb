{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# import image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_PATH = \"./data/train/\"\n",
    "TEST_IMG_PATH = \"./data/test/\"\n",
    "LABELS_CSV_PATH = \"./data/train.csv\"\n",
    "SAMPLE_SUB_PATH = \"./data/sample_submission.csv\"\n",
    "\n",
    "class CactusDataset(Dataset):\n",
    "    \"\"\"Cactus identification dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.        \n",
    "            dataframe (pandas.core.frame.DataFrame): Pandas dataframe obtained\n",
    "                by read_csv().\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.labels_frame = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.labels_frame.id[idx]) \n",
    "        image = Image.open(img_name)\n",
    "        label = self.labels_frame.has_cactus[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return [image, label] \n",
    "    \n",
    "    \n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe = pd.read_csv(LABELS_CSV_PATH)\n",
    "cut = int(len(dframe)*0.95)\n",
    "train, test = np.split(dframe, [cut], axis=0)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "train_ds = CactusDataset(TRAIN_IMG_PATH, train, train_transforms)\n",
    "test_ds = CactusDataset(TRAIN_IMG_PATH, test, test_transforms)\n",
    "datasets = {\"train\": train_ds, \"val\": test_ds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_ds, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "testloader = DataLoader(test_ds, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.003\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCNN(nn.Module):\n",
    "\n",
    "    #Our batch shape for input x is (32, 32, 3)\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Init method\n",
    "        '''\n",
    "        super(ModelCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=0, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 25 * 25, 512)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the model.\n",
    "        INPUT:\n",
    "            x - input data\n",
    "        '''\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        \n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 128 * 25 * 25)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = F.log_softmax(self.dropout1(x), dim = 1)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelCNN()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1.. Train loss: 4.795.. Test loss: 1.051.. Test accuracy: 0.749\n",
      "Epoch 1/1.. Train loss: 2.514.. Test loss: 1.611.. Test accuracy: 0.748\n",
      "Epoch 1/1.. Train loss: 2.751.. Test loss: 1.492.. Test accuracy: 0.749\n",
      "Epoch 1/1.. Train loss: 2.897.. Test loss: 1.237.. Test accuracy: 0.749\n",
      "Epoch 1/1.. Train loss: 2.701.. Test loss: 1.179.. Test accuracy: 0.749\n",
      "Epoch 1/1.. Train loss: 2.059.. Test loss: 1.076.. Test accuracy: 0.748\n",
      "Epoch 1/1.. Train loss: 2.277.. Test loss: 1.437.. Test accuracy: 0.811\n",
      "Epoch 1/1.. Train loss: 2.373.. Test loss: 0.807.. Test accuracy: 0.870\n",
      "Epoch 1/1.. Train loss: 2.237.. Test loss: 1.479.. Test accuracy: 0.816\n",
      "Epoch 1/1.. Train loss: 2.315.. Test loss: 0.956.. Test accuracy: 0.885\n",
      "Epoch 1/1.. Train loss: 2.062.. Test loss: 0.999.. Test accuracy: 0.873\n",
      "Epoch 1/1.. Train loss: 2.045.. Test loss: 1.315.. Test accuracy: 0.892\n",
      "Epoch 1/1.. Train loss: 2.381.. Test loss: 0.993.. Test accuracy: 0.888\n",
      "Epoch 1/1.. Train loss: 2.310.. Test loss: 1.053.. Test accuracy: 0.878\n",
      "Epoch 1/1.. Train loss: 2.349.. Test loss: 1.094.. Test accuracy: 0.854\n",
      "Epoch 1/1.. Train loss: 2.507.. Test loss: 1.050.. Test accuracy: 0.843\n",
      "Epoch 1/1.. Train loss: 2.567.. Test loss: 1.144.. Test accuracy: 0.860\n",
      "Epoch 1/1.. Train loss: 2.075.. Test loss: 0.839.. Test accuracy: 0.813\n",
      "Epoch 1/1.. Train loss: 2.238.. Test loss: 0.849.. Test accuracy: 0.872\n",
      "Epoch 1/1.. Train loss: 2.552.. Test loss: 1.263.. Test accuracy: 0.854\n",
      "Epoch 1/1.. Train loss: 2.055.. Test loss: 0.888.. Test accuracy: 0.873\n",
      "Epoch 1/1.. Train loss: 2.788.. Test loss: 0.882.. Test accuracy: 0.887\n",
      "Epoch 1/1.. Train loss: 1.988.. Test loss: 1.055.. Test accuracy: 0.877\n",
      "Epoch 1/1.. Train loss: 2.246.. Test loss: 0.844.. Test accuracy: 0.888\n",
      "Epoch 1/1.. Train loss: 2.726.. Test loss: 1.147.. Test accuracy: 0.819\n",
      "Epoch 1/1.. Train loss: 2.126.. Test loss: 0.956.. Test accuracy: 0.831\n",
      "Epoch 1/1.. Train loss: 2.289.. Test loss: 0.880.. Test accuracy: 0.857\n",
      "Epoch 1/1.. Train loss: 2.621.. Test loss: 1.137.. Test accuracy: 0.828\n",
      "Epoch 1/1.. Train loss: 2.296.. Test loss: 1.062.. Test accuracy: 0.833\n",
      "Epoch 1/1.. Train loss: 2.267.. Test loss: 0.815.. Test accuracy: 0.849\n",
      "Epoch 1/1.. Train loss: 2.219.. Test loss: 0.983.. Test accuracy: 0.844\n",
      "Epoch 1/1.. Train loss: 2.260.. Test loss: 1.098.. Test accuracy: 0.866\n",
      "Epoch 1/1.. Train loss: 2.002.. Test loss: 0.800.. Test accuracy: 0.886\n",
      "Epoch 1/1.. Train loss: 1.855.. Test loss: 0.631.. Test accuracy: 0.885\n",
      "Epoch 1/1.. Train loss: 2.487.. Test loss: 1.189.. Test accuracy: 0.896\n",
      "Epoch 1/1.. Train loss: 2.334.. Test loss: 0.776.. Test accuracy: 0.900\n",
      "Epoch 1/1.. Train loss: 2.001.. Test loss: 0.580.. Test accuracy: 0.893\n",
      "Epoch 1/1.. Train loss: 2.536.. Test loss: 1.097.. Test accuracy: 0.872\n",
      "Epoch 1/1.. Train loss: 2.212.. Test loss: 0.601.. Test accuracy: 0.883\n",
      "Epoch 1/1.. Train loss: 2.587.. Test loss: 0.866.. Test accuracy: 0.815\n",
      "Epoch 1/1.. Train loss: 2.837.. Test loss: 1.423.. Test accuracy: 0.752\n",
      "Epoch 1/1.. Train loss: 2.375.. Test loss: 0.783.. Test accuracy: 0.817\n",
      "Epoch 1/1.. Train loss: 2.484.. Test loss: 1.073.. Test accuracy: 0.748\n",
      "Epoch 1/1.. Train loss: 1.977.. Test loss: 0.638.. Test accuracy: 0.801\n",
      "Epoch 1/1.. Train loss: 2.269.. Test loss: 1.003.. Test accuracy: 0.808\n",
      "Epoch 1/1.. Train loss: 2.226.. Test loss: 1.029.. Test accuracy: 0.749\n",
      "Epoch 1/1.. Train loss: 2.370.. Test loss: 1.179.. Test accuracy: 0.737\n",
      "Epoch 1/1.. Train loss: 2.504.. Test loss: 1.218.. Test accuracy: 0.748\n",
      "Epoch 1/1.. Train loss: 2.359.. Test loss: 0.924.. Test accuracy: 0.842\n",
      "Epoch 1/1.. Train loss: 2.508.. Test loss: 0.796.. Test accuracy: 0.826\n",
      "Epoch 1/1.. Train loss: 2.661.. Test loss: 1.106.. Test accuracy: 0.749\n",
      "Epoch 1/1.. Train loss: 2.013.. Test loss: 1.060.. Test accuracy: 0.810\n",
      "Epoch 1/1.. Train loss: 2.331.. Test loss: 1.117.. Test accuracy: 0.797\n",
      "Epoch 1/1.. Train loss: 2.480.. Test loss: 1.199.. Test accuracy: 0.762\n",
      "Epoch 1/1.. Train loss: 2.323.. Test loss: 1.147.. Test accuracy: 0.748\n",
      "Epoch 1/1.. Train loss: 2.182.. Test loss: 0.840.. Test accuracy: 0.832\n",
      "Epoch 1/1.. Train loss: 1.971.. Test loss: 0.693.. Test accuracy: 0.834\n",
      "Epoch 1/1.. Train loss: 2.753.. Test loss: 1.329.. Test accuracy: 0.841\n",
      "Epoch 1/1.. Train loss: 2.013.. Test loss: 0.798.. Test accuracy: 0.841\n",
      "Epoch 1/1.. Train loss: 2.499.. Test loss: 1.091.. Test accuracy: 0.841\n",
      "Epoch 1/1.. Train loss: 2.340.. Test loss: 1.236.. Test accuracy: 0.745\n",
      "Epoch 1/1.. Train loss: 2.394.. Test loss: 1.053.. Test accuracy: 0.812\n",
      "Epoch 1/1.. Train loss: 2.421.. Test loss: 0.826.. Test accuracy: 0.748\n",
      "Epoch 1/1.. Train loss: 2.416.. Test loss: 0.932.. Test accuracy: 0.825\n",
      "Epoch 1/1.. Train loss: 2.155.. Test loss: 0.880.. Test accuracy: 0.833\n",
      "Epoch 1/1.. Train loss: 2.478.. Test loss: 0.859.. Test accuracy: 0.746\n",
      "Epoch 1/1.. Train loss: 2.146.. Test loss: 0.900.. Test accuracy: 0.747\n",
      "Epoch 1/1.. Train loss: 2.503.. Test loss: 1.154.. Test accuracy: 0.838\n",
      "Epoch 1/1.. Train loss: 2.082.. Test loss: 0.860.. Test accuracy: 0.857\n",
      "Epoch 1/1.. Train loss: 2.640.. Test loss: 0.922.. Test accuracy: 0.850\n",
      "Epoch 1/1.. Train loss: 2.391.. Test loss: 0.870.. Test accuracy: 0.869\n",
      "Epoch 1/1.. Train loss: 2.127.. Test loss: 0.697.. Test accuracy: 0.885\n",
      "Epoch 1/1.. Train loss: 2.614.. Test loss: 1.314.. Test accuracy: 0.850\n",
      "Epoch 1/1.. Train loss: 2.939.. Test loss: 0.987.. Test accuracy: 0.872\n",
      "Epoch 1/1.. Train loss: 2.086.. Test loss: 0.679.. Test accuracy: 0.847\n",
      "Epoch 1/1.. Train loss: 2.486.. Test loss: 1.040.. Test accuracy: 0.812\n",
      "Epoch 1/1.. Train loss: 2.449.. Test loss: 0.983.. Test accuracy: 0.748\n",
      "Epoch 1/1.. Train loss: 2.483.. Test loss: 0.885.. Test accuracy: 0.749\n",
      "Epoch 1/1.. Train loss: 2.076.. Test loss: 1.034.. Test accuracy: 0.804\n",
      "Epoch 1/1.. Train loss: 2.335.. Test loss: 1.222.. Test accuracy: 0.736\n",
      "Epoch 1/1.. Train loss: 2.214.. Test loss: 0.853.. Test accuracy: 0.749\n",
      "Epoch 1/1.. Train loss: 2.221.. Test loss: 0.935.. Test accuracy: 0.837\n",
      "Epoch 1/1.. Train loss: 2.306.. Test loss: 0.884.. Test accuracy: 0.847\n",
      "Epoch 1/1.. Train loss: 2.451.. Test loss: 0.798.. Test accuracy: 0.845\n",
      "Epoch 1/1.. Train loss: 2.868.. Test loss: 1.125.. Test accuracy: 0.847\n",
      "Epoch 1/1.. Train loss: 2.023.. Test loss: 0.882.. Test accuracy: 0.817\n",
      "Epoch 1/1.. Train loss: 2.558.. Test loss: 1.207.. Test accuracy: 0.749\n",
      "Epoch 1/1.. Train loss: 2.527.. Test loss: 1.053.. Test accuracy: 0.748\n",
      "Epoch 1/1.. Train loss: 2.290.. Test loss: 0.593.. Test accuracy: 0.836\n",
      "Epoch 1/1.. Train loss: 2.137.. Test loss: 0.637.. Test accuracy: 0.847\n",
      "Epoch 1/1.. Train loss: 2.677.. Test loss: 0.962.. Test accuracy: 0.857\n",
      "Epoch 1/1.. Train loss: 2.877.. Test loss: 1.050.. Test accuracy: 0.749\n",
      "Epoch 1/1.. Train loss: 2.295.. Test loss: 0.717.. Test accuracy: 0.834\n",
      "Epoch 1/1.. Train loss: 2.182.. Test loss: 0.697.. Test accuracy: 0.876\n",
      "Epoch 1/1.. Train loss: 1.976.. Test loss: 0.978.. Test accuracy: 0.872\n",
      "Epoch 1/1.. Train loss: 2.204.. Test loss: 0.616.. Test accuracy: 0.885\n",
      "Epoch 1/1.. Train loss: 2.265.. Test loss: 0.735.. Test accuracy: 0.892\n",
      "Epoch 1/1.. Train loss: 2.050.. Test loss: 1.009.. Test accuracy: 0.892\n",
      "Epoch 1/1.. Train loss: 2.106.. Test loss: 0.799.. Test accuracy: 0.892\n",
      "Epoch 1/1.. Train loss: 2.214.. Test loss: 0.769.. Test accuracy: 0.870\n",
      "Epoch 1/1.. Train loss: 2.205.. Test loss: 0.712.. Test accuracy: 0.870\n",
      "Epoch 1/1.. Train loss: 2.583.. Test loss: 1.251.. Test accuracy: 0.885\n",
      "Epoch 1/1.. Train loss: 2.166.. Test loss: 0.777.. Test accuracy: 0.881\n",
      "Epoch 1/1.. Train loss: 2.179.. Test loss: 0.631.. Test accuracy: 0.896\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 5\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    \n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "output_df = pd.DataFrame(index=submission_df.index, columns=submission_df.keys() )\n",
    "output_df['id'] = submission_df['id']\n",
    "submission_df['target'] =  [0] * len(submission_df)\n",
    "\n",
    "tdata_transform = transforms.Compose([transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "submission_ds = CactusDataset(TEST_IMG_PATH, submission_df, tdata_transform)\n",
    "\n",
    "sub_loader = DataLoader(submission_ds, batch_size=1,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "def test_sumission(model):\n",
    "    since = time.time()\n",
    "    sub_outputs = []\n",
    "    model.train(False)  # Set model to evaluate mode\n",
    "    # Iterate over data.\n",
    "    prediction = []\n",
    "    for data in sub_loader:\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        prediction.append(int(pred))\n",
    "      \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Run complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run complete in 2m 49s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000940378805c44108d287872b2f04ce.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0017242f54ececa4512b4d7937d1e21e.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ee6d8564003107853118ab87df407.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002e175c3c1e060769475f52182583d0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0036e44a7e8f7218e9bc7bf8137e4943.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  has_cactus\n",
       "0  000940378805c44108d287872b2f04ce.jpg           1\n",
       "1  0017242f54ececa4512b4d7937d1e21e.jpg           1\n",
       "2  001ee6d8564003107853118ab87df407.jpg           0\n",
       "3  002e175c3c1e060769475f52182583d0.jpg           0\n",
       "4  0036e44a7e8f7218e9bc7bf8137e4943.jpg           1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "sub['has_cactus'] = test_sumission(model)\n",
    "sub.to_csv('submission_conv.csv', index= False)\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
